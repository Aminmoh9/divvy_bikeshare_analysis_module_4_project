{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dec20a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the cleaned dataset. Combined dataset shape: (3818004, 13)\n"
     ]
    }
   ],
   "source": [
    "#We need to first clean and concatenate the datasets from 2019\n",
    "#Import libararies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "#Load the 2019 datasets\n",
    "df1=pd.read_csv(\"./datasets_2019/20190103-divvy-tripdata.csv\")\n",
    "df2=pd.read_csv(\"./datasets_2019/20190406-divvy-tripdata.csv\")\n",
    "df3=pd.read_csv(\"./datasets_2019/20190709-divvy-tripdata.csv\")\n",
    "df4=pd.read_csv(\"./datasets_2019/20191012-divvy-tripdata.csv\")\n",
    "\n",
    "\n",
    "# Rename the colum names of df2 to match the rest\n",
    "df2 = df2.rename(columns={'01 - Rental Details Rental ID': 'trip_id', '01 - Rental Details Local Start Time': 'start_time',\n",
    "                          '01 - Rental Details Local End Time':'end_time','01 - Rental Details Bike ID': 'bikeid',\n",
    "                          '01 - Rental Details Duration In Seconds Uncapped':'tripduration', '03 - Rental Start Station ID':'from_station_id', \n",
    "                          '03 - Rental Start Station Name':'from_station_name', '02 - Rental End Station ID':'to_station_id',\n",
    "                          '02 - Rental End Station Name':'to_station_name', 'User Type':'usertype','Member Gender':'gender',\n",
    "                          '05 - Member Details Member Birthday Year':'birthyear'\n",
    "                          })\n",
    "\n",
    "# Concatenate all the datasets\n",
    "combined_df_2019= pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "\n",
    "#Change the values in column user type (Subscriber to member and customer to casual)\n",
    "\n",
    "combined_df_2019['usertype']= combined_df_2019['usertype'].replace({'Subscriber': 'member',\n",
    "    'Customer': 'casual'})\n",
    "\n",
    "# Rename columns to match the datasets from 2020 to 2024\n",
    "combined_df_2019 = combined_df_2019.rename(columns={\n",
    "    'trip_id': 'ride_id',\n",
    "    'start_time': 'started_at',\n",
    "    'end_time': 'ended_at',\n",
    "    'from_station_name': 'start_station_name',\n",
    "    'from_station_id': 'start_station_id',\n",
    "    'to_station_name': 'end_station_name',\n",
    "    'to_station_id': 'end_station_id',\n",
    "    'usertype': 'member_casual'\n",
    "})\n",
    "\n",
    "# Add missing columns with empty or NaN values to match the column names of the datasest from 2020 to 2024\n",
    "combined_df_2019['rideable_type'] = np.nan\n",
    "combined_df_2019['start_lat'] = np.nan\n",
    "combined_df_2019['start_lng'] = np.nan\n",
    "combined_df_2019['end_lat'] = np.nan\n",
    "combined_df_2019['end_lng'] = np.nan\n",
    "\n",
    "# Drop columns that don't exist in the datasest from 2020 to 2024\n",
    "combined_df_2019= combined_df_2019.drop(['bikeid', 'tripduration', 'gender', 'birthyear'], axis=1)\n",
    "\n",
    "\n",
    " # Reorder columns to match the datasets from 2020 to 2024\n",
    "desired_order = [\n",
    "    'ride_id', 'rideable_type', 'started_at', 'ended_at',\n",
    "    'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id',\n",
    "    'start_lat', 'start_lng', 'end_lat', 'end_lng', 'member_casual'\n",
    "]\n",
    "\n",
    "combined_df_2019  = combined_df_2019[desired_order]\n",
    "\n",
    "#Save the cleaned dataset to CSV\n",
    "combined_df_2019.to_csv('datasets_2019_2024/2019-divvy-tripdata_cleaned.csv', index=False)\n",
    "print(\"Successfully saved the cleaned dataset. Combined dataset shape:\", combined_df_2019.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17ae5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset shape before cleaning: (30202912, 13)\n",
      "Exported cleaned dataset: divvy_cleaned_2019_2024.csv . Cleaned datase shape after cleaning: (30202834, 20)\n"
     ]
    }
   ],
   "source": [
    "# Here we access all the datasets and try to concatenate them\n",
    "# Path to datasets folder\n",
    "folder_path = \"./datasets_2019_2024/\"\n",
    "\n",
    "#Pattern to match dataset for 2019\n",
    "file_2019 = folder_path +\"2019-divvy-tripdata_cleaned.csv\"\n",
    "df_2019 = pd.read_csv(file_2019)\n",
    "\n",
    "#Pattern to match all CSV files for year 2020 to 2024\n",
    "file_pattern = folder_path + \"202*-divvy-tripdata.csv\"\n",
    "\n",
    "#Get list of matching files\n",
    "all_files = glob.glob(file_pattern)\n",
    "\n",
    "#List to hold dataframes\n",
    "df_list= [df_2019]\n",
    "\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    df_list.append(df)\n",
    "\n",
    "# Combine all datasets into one dataframe\n",
    "combined_df = pd.concat(df_list, ignore_index= True)\n",
    "print(\"Combined dataset shape before cleaning:\", combined_df.shape)\n",
    "\n",
    "# Filter invalid coordinates\n",
    "combined_df = combined_df[(combined_df['start_lat'].between(41.5, 42.2) | combined_df['start_lat'].isna()) &\n",
    "    (combined_df['start_lng'].between(-88.1, -87) | combined_df['start_lng'].isna()) &\n",
    "    (combined_df['end_lat'].between(41.5, 42.2) | combined_df['end_lat'].isna()) &\n",
    "    (combined_df['end_lng'].between(-88.1, -87) | combined_df['end_lng'].isna())]\n",
    "\n",
    "#Fill rideable type's missing values with unknown\n",
    "combined_df['rideable_type'] = combined_df['rideable_type'].fillna('unknown')\n",
    "\n",
    "#Fill the missing stations names with unknown\n",
    "combined_df['start_station_name'] = combined_df['start_station_name'].fillna('Unknown')\n",
    "combined_df['end_station_name'] = combined_df['end_station_name'].fillna('Unknown')\n",
    "combined_df['start_station_id'] = combined_df['start_station_id'].fillna('Unknown')\n",
    "combined_df['end_station_id'] = combined_df['end_station_id'].fillna('Unknown')\n",
    "\n",
    "#Split the dataset to convert started_at and ended_at columns to datetime type correctly\n",
    "combined_df_part1 = combined_df.iloc[:26036568].copy()\n",
    "combined_df_part2 = combined_df.iloc[26036568:].copy()\n",
    "\n",
    "# First part — no microseconds-convert it to datetime\n",
    "combined_df_part1['started_at'] = pd.to_datetime(combined_df_part1['started_at'], errors='coerce')\n",
    "combined_df_part1['ended_at'] = pd.to_datetime(combined_df_part1['ended_at'], errors='coerce')\n",
    "\n",
    "# Second part — has microseconds-convert to datetime with the explicit parse method to parse millisecond\n",
    "combined_df_part2['started_at'] = pd.to_datetime(combined_df_part2['started_at'], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "combined_df_part2['ended_at'] = pd.to_datetime(combined_df_part2['ended_at'], format='%Y-%m-%d %H:%M:%S.%f', errors='coerce')\n",
    "\n",
    "#Conctanate the dfs\n",
    "df_cleaned = pd.concat([combined_df_part1, combined_df_part2], ignore_index=True)\n",
    "\n",
    "#Create the neccessary columns for performing aggregations\n",
    "df_cleaned['year'] = df_cleaned['started_at'].dt.year\n",
    "df_cleaned['month'] = df_cleaned['started_at'].dt.to_period('M').astype(str)  # e.g., '2024-01'\n",
    "df_cleaned['month_number'] = df_cleaned['started_at'].dt.month\n",
    "df_cleaned['month_name'] = df_cleaned['started_at'].dt.strftime('%B')         # e.g., 'January'\n",
    "df_cleaned['day_of_week'] = df_cleaned['started_at'].dt.day_name()\n",
    "df_cleaned['hour'] = df_cleaned['started_at'].dt.hour\n",
    "df_cleaned['ride_duration_min'] = ((df_cleaned['ended_at'] - df_cleaned['started_at']).dt.total_seconds() / 60).round(2) # round(2)\n",
    "\n",
    "#Save cleaned dataset\n",
    "df_cleaned.to_csv('cleaned_dataset_2019_2024/divvy_2019_2024_cleaned.csv',index=False)\n",
    "print(\"Exported cleaned dataset: divvy_cleaned_2019_2024.csv . Cleaned datase shape after cleaning:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642da8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned CSV loaded for performing the aggregations.\n"
     ]
    }
   ],
   "source": [
    "#Load the cleaned final csv for perfroming the aggregation to avoid running the whole cleaning code every time\n",
    "\n",
    "df_cleaned= pd.read_csv('cleaned_dataset_2019_2024/divvy_2019_2024_cleaned.csv', low_memory= False)\n",
    "print('Final cleaned CSV loaded for performing the aggregations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "214fb2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: aggregations/agg_overview.csv\n",
      "Saved: aggregations/agg_total_year.csv\n",
      "Saved: aggregations/agg_monthly.csv\n",
      "Saved: aggregations/agg_duration.csv\n",
      "Saved: aggregations/agg_ride_type.csv\n",
      "Saved: aggregations/agg_day_hour.csv\n",
      "Saved: aggregations/agg_start_station.csv\n",
      "Saved: aggregations/agg_end_station.csv\n",
      "Saved: aggregations/agg_routes.csv\n",
      "Saved: aggregations/agg_duration_member.csv\n",
      "Saved: aggregations/agg_zip_routes.csv\n",
      "All aggrepagtions have been saved to the folder aggregations.\n"
     ]
    }
   ],
   "source": [
    "#KPI Overview - rider per year by user type\n",
    "agg_overview =(df_cleaned.groupby(['year', 'member_casual']).agg(number_of_rides =('ride_id', 'count')).reset_index())\n",
    "\n",
    "# Total rides and growth by year\n",
    "agg_total_year = (df_cleaned.groupby('year').agg(total_rides=('ride_id','count')).reset_index())\n",
    "agg_total_year['growth_%'] = (agg_total_year['total_rides'].pct_change()* 100).round(2)\n",
    "\n",
    "#Monthly Ride Trends\n",
    "agg_monthly = (df_cleaned.groupby(['year', 'month', 'month_number', 'month_name', 'member_casual']).agg(number_of_rides=('ride_id', 'count'))\n",
    "               .reset_index().sort_values(['year', 'month_number']))\n",
    "\n",
    "#Ride duration by user and bike type\n",
    "agg_duration=(df_cleaned.groupby(['year','member_casual', 'rideable_type']).agg(avg_duration_min=('ride_duration_min','mean')).reset_index())\n",
    "\n",
    "#Ride Type Usage\n",
    "agg_ride_type=(df_cleaned.groupby(['year','member_casual','rideable_type']).agg(number_of_rides=('ride_id','count')).reset_index())\n",
    "\n",
    "#Weekday annd hour usage\n",
    "agg_day_hour= (df_cleaned.groupby(['day_of_week','hour']).agg(number_of_rides=('ride_id','count')).reset_index())\n",
    "\n",
    "\n",
    "#Top start and stations by year\n",
    "\n",
    "df_filtered = df_cleaned[\n",
    "    (df_cleaned['start_station_name'].notna()) &\n",
    "    (df_cleaned['end_station_name'].notna()) &\n",
    "    (df_cleaned['start_station_name'] != 'Unknown') &\n",
    "    (df_cleaned['end_station_name'] != 'Unknown')\n",
    "].copy()\n",
    "\n",
    "agg_start_station = (\n",
    "    df_filtered.groupby(['start_station_name', 'year'])\n",
    "    .agg(number_of_rides=('ride_id', 'count'))\n",
    "    .reset_index()\n",
    "    .sort_values(['year', 'number_of_rides'], ascending=[True, False])\n",
    "    )\n",
    "agg_end_station = (\n",
    "    df_filtered.groupby(['end_station_name', 'year'])\n",
    "    .agg(number_of_rides=('ride_id', 'count'))\n",
    "    .reset_index()\n",
    "    .sort_values(['year', 'number_of_rides'], ascending=[True, False])\n",
    "    )\n",
    "\n",
    "#Top routes (start to end station pairs)\n",
    "# Create the route column\n",
    "df_filtered.loc[:, 'route'] = df_filtered['start_station_name'] + ' → ' + df_filtered['end_station_name']\n",
    "\n",
    "# Aggregate top 10 routes per year\n",
    "agg_routes = (\n",
    "    df_filtered.groupby(['route', 'year', 'member_casual'])\n",
    "    .agg(number_of_rides=('ride_id', 'count'))\n",
    "    .reset_index()\n",
    "    .sort_values(['year', 'member_casual', 'number_of_rides'], ascending=[True, True, False])\n",
    "    .groupby(['year', 'member_casual'])\n",
    "    .head(10)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "#Ride Duration Segmentation\n",
    "# Create the duration buckets (adjusted for df_cleaned)\n",
    "bins = [0, 5, 10, 20, 30, 60, 120, 240, 1440]\n",
    "labels = ['<5 min', '5-10', '10-20', '20-30', '30-60', '1-2 hr', '2-4 hr', '>4 hr']\n",
    "\n",
    "df_cleaned['duration_bucket'] = pd.cut(df_cleaned['ride_duration_min'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Group by duration_bucket and member_casual, count rides\n",
    "agg_duration_member = (\n",
    "    df_cleaned.groupby(['duration_bucket','rideable_type','year', 'member_casual'],observed=True)\n",
    "    .agg(number_of_rides=('ride_id', 'count'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Load Chicago Zip cod csv file for coordinate to ZIP Code Mapping\n",
    "zip_df = pd.read_csv(\"data/chicago_zip_codes.csv\", dtype={'ZIP_Code': str}) \n",
    "\n",
    "#Import Library\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# Convert to radians for BallTree\n",
    "zip_coords = np.radians(zip_df[['Latitude', 'Longitude']]) \n",
    "\n",
    "#Build BallTree for fast nearest neigbor search\n",
    "tree = BallTree(zip_coords, metric='haversine')\n",
    "\n",
    "#Assign ZIP to start coordinates\n",
    "# Prepare start coords and convert to radians\n",
    "start_coords = df_cleaned[['start_lat', 'start_lng']].dropna()\n",
    "start_coords_rad = np.radians(start_coords)\n",
    "\n",
    "# Query nearest ZIP\n",
    "distances_start, indices_start = tree.query(start_coords_rad, k=1)\n",
    "\n",
    "# Map indices to ZIP codes\n",
    "start_zip_matches = zip_df.iloc[indices_start.flatten()]['ZIP_Code'].values\n",
    "\n",
    "# Assign ZIPs to df_cleaned\n",
    "df_cleaned['start_zip'] = np.nan\n",
    "df_cleaned['start_zip'] = df_cleaned['start_zip'].astype(object)\n",
    "df_cleaned.loc[start_coords.index, 'start_zip'] = start_zip_matches\n",
    "\n",
    "#Assign ZIP to end coordinates\n",
    "# Prepare end coords and convert to radians\n",
    "end_coords = df_cleaned[['end_lat', 'end_lng']].dropna()\n",
    "end_coords_rad = np.radians(end_coords)\n",
    "\n",
    "# Query nearest ZIP\n",
    "distances_end, indices_end = tree.query(end_coords_rad, k=1)\n",
    "\n",
    "# Map indices to ZIP codes\n",
    "end_zip_matches = zip_df.iloc[indices_end.flatten()]['ZIP_Code'].values\n",
    "\n",
    "# Assign ZIPs to df_cleaned\n",
    "df_cleaned['end_zip'] = np.nan\n",
    "df_cleaned['end_zip'] = df_cleaned['end_zip'].astype(object)\n",
    "df_cleaned.loc[end_coords.index, 'end_zip'] = end_zip_matches\n",
    "\n",
    "# Fill missing ZIPs with 'Unknown'\n",
    "df_cleaned['start_zip'] = df_cleaned['start_zip'].fillna('Unknown')\n",
    "df_cleaned['end_zip'] = df_cleaned['end_zip'].fillna('Unknown')\n",
    "\n",
    "#Geographic insights: popular ZIP routes\n",
    "agg_zip_routes = (\n",
    "    df_cleaned\n",
    "    .groupby(['start_zip', 'end_zip', 'year'])\n",
    "    .agg(number_of_rides=('ride_id', 'count'),\n",
    "         avg_ride_duration_min=('ride_duration_min', 'mean'))\n",
    "    .reset_index()\n",
    ")\n",
    "agg_zip_routes['avg_ride_duration_min'] = agg_zip_routes['avg_ride_duration_min'].round(2)\n",
    "\n",
    "#Cleaning the ZIP formats\n",
    "agg_zip_routes['start_zip'] = agg_zip_routes['start_zip'].astype(str).str.replace(r'\\.0$', '', regex=True).str.zfill(5)\n",
    "agg_zip_routes['end_zip'] = agg_zip_routes['end_zip'].astype(str).str.replace(r'\\.0$', '', regex=True).str.zfill(5)\n",
    "\n",
    "#Removing the unknows from the aggregate table\n",
    "agg_zip_routes = agg_zip_routes[\n",
    "    (agg_zip_routes['start_zip'] != 'Unknown') &\n",
    "    (agg_zip_routes['end_zip'] != 'Unknown')\n",
    "]\n",
    "\n",
    "#Make sure the ZIP codes match the pattern of 5 digits\n",
    "agg_zip_routes = agg_zip_routes[\n",
    "    agg_zip_routes['start_zip'].str.match(r'^\\d{5}$') &\n",
    "    agg_zip_routes['end_zip'].str.match(r'^\\d{5}$')\n",
    "]\n",
    "\n",
    "#Save aggregations to csv\n",
    "agg_overview.to_csv(\"aggregations/agg_overview.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_overview.csv\")\n",
    "\n",
    "agg_total_year.to_csv(\"aggregations/agg_total_year.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_total_year.csv\")\n",
    "\n",
    "agg_monthly.to_csv(\"aggregations/agg_monthly.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_monthly.csv\")\n",
    "\n",
    "agg_duration.to_csv(\"aggregations/agg_duration.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_duration.csv\")\n",
    "\n",
    "agg_ride_type.to_csv(\"aggregations/agg_ride_type.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_ride_type.csv\")\n",
    "\n",
    "agg_day_hour.to_csv(\"aggregations/agg_day_hour.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_day_hour.csv\")\n",
    "\n",
    "agg_start_station.to_csv(\"aggregations/agg_start_station.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_start_station.csv\")\n",
    "\n",
    "agg_end_station.to_csv(\"aggregations/agg_end_station.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_end_station.csv\")\n",
    "\n",
    "agg_routes.to_csv(\"aggregations/agg_routes.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_routes.csv\")\n",
    "\n",
    "agg_duration_member.to_csv(\"aggregations/agg_duration_member.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_duration_member.csv\")\n",
    "\n",
    "agg_zip_routes.to_csv(\"aggregations/agg_zip_routes.csv\", index=False)\n",
    "print(\"Saved: aggregations/agg_zip_routes.csv\")\n",
    "print('All aggrepagtions have been saved to the folder aggregations.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
